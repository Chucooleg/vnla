# Vision-based Navigation with Language-based Assistance via Imitation Learning with Indirect Intervention

Authors: Khanh Nguyen, Debadeepta Dey, Chris Brockett, Bill Dolan.

This repo contains code and data-downloading scripts for the paper [Vision-based Navigation with Language-based Assistance via Imitation Learning with Indirect Intervention](https://arxiv.org/abs/1812.04155). We present Vision-based Navigation with Language-based Assistance (VNLA, pronounced as *"Vanilla"*), a grounded vision-language task where an agent with visual perception is guided via language to find objects in photorealistic indoor environments. 

![Concept](example.png)

0. Clone this repo `git clone git@github.com:debadeepta/learningtoask.git`. 
1. [Download data](https://github.com/debadeepta/learningtoask/tree/master/data). 
2. [Setup simulator](https://github.com/debadeepta/learningtoask/tree/master/code). 
3. [Run experiments](https://github.com/debadeepta/learningtoask/tree/master/code/tasks/VNLA). 

Please create Github issues or email kxnguyen@cs.umd.edu for any questions or feedback. 

