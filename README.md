# Vision-based Navigation with Language-based Assistance via Imitation Learning with Indirect Intervention

This repo contains code and data-downloading scripts for the paper [Vision-based Navigation with Language-based Assistance via Imitation Learning with Indirect Intervention](https://arxiv.org/abs/1812.04155). We present Vision-based Navigation with Language-based Assistance (VNLA), a grounded vision-language task where an agent with visual perception is guided via language to find objects in photorealistic indoor environments. 

![Concept](example.png)

0. CLone this repo `git clone git@github.com:debadeepta/learningtoask.git`. 
1. [Download data](https://github.com/debadeepta/learningtoask/tree/master/data). 
2. [Setup simulators](https://github.com/debadeepta/learningtoask/tree/master/code). 
3. Run experiments. 

Please create Github issues or email kxnguyen@cs.umd.edu for any questions or feedback. 
